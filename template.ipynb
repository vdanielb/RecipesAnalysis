{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Your Title Here\n",
    "\n",
    "**Name(s)**: Daniel Budidharma, Tristan Leo\n",
    "\n",
    "**Website Link**: (your website link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-31T23:36:28.652554Z",
     "start_time": "2019-10-31T23:36:27.180520Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "pd.options.plotting.backend = 'plotly'\n",
    "\n",
    "# from dsc80_utils import * # Feel free to uncomment and use this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's load in the dataset and take a look at it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recipes = pd.read_csv('data/RAW_recipes.csv')\n",
    "interactions = pd.read_csv('data/RAW_interactions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(recipes.head())\n",
    "display(interactions.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Data Cleaning and Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at a particular row in `interactions`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(interactions.iloc[3:4])\n",
    "print(interactions['review'].iloc[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the lowest possible rating a user could give is 1 star. So how does this recipe have a rating of 0? It turns out that that means the reviewer just didn't leave a rating. Like the review in this particular row says, \"...so I will not rate\". It makes sense then to replace these values with NaN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactions['rating'] = interactions['rating'].replace(0, np.nan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another thing we should notice is that the values in the tags column in `recipes` isn't actually a list. This is also true for other columns with values that look like lists. They're actually strings! To convert them into a list, we define a function and apply it to all those columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_col_string_to_list(df, col):\n",
    "    translation_table = str.maketrans({\"[\": \"\", \n",
    "                                   \"]\": \"\",\n",
    "                                    \"\\'\":\"\"})\n",
    "    df[col] = df[col].str.translate(translation_table).str.split(', ')\n",
    "\n",
    "for col in ['tags','nutrition', 'steps', 'ingredients']:\n",
    "    convert_col_string_to_list(recipes, col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's verify they're lists now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The type of the value is: \",type(recipes['tags'].iloc[4268]))\n",
    "(recipes['tags'].iloc[4268])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can actually perform list operations on those columns. Next, we're interested in finding the average rating per recipe. To do that we'll first have to merge the recipes and ratings dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recipes_with_ratings = recipes.merge(interactions, left_on='id', right_on='recipe_id',how='left')\n",
    "recipes_with_ratings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recipes_with_ratings['average_rating'] = recipes_with_ratings.groupby('id')['rating'].transform(lambda x: x.mean())\n",
    "recipes_with_ratings.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distribution of ratings should theoretically look something like a normal distribution, with most people rating 3 stars for average satisfaction, while few people would have extreme experiences that would warrant a 5 star or 1 star. Does our ratings column look like a normal distribution? Let's check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.histogram(interactions, x=\"rating\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Surprisingly a lot of 5s. Does this mean every recipe on food.com is a masterpiece? Probably not. It just means people are generous with ratings. \n",
    "<br> Still, this isn't good because it means the average rating doesn't tell us much about the actual quality of the recipe compared to other recipes. If everything is 5 stars, how do I know which recipe is better than the other? It is for this reason that we think any analysis involving the average rating probably won't be very useful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do something similar with number of reviews of each recipe. We define a function to get the number of reviews of each recipe id. And then we plot a histogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_reviews(id):\n",
    "    return interactions[interactions['user_id'] == id].shape[0]\n",
    "recipes['num_reviews'] = recipes['id'].apply(get_num_reviews)\n",
    "px.histogram(recipes['num_reviews'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, an overwhelming majority of recipes have 0 reviews. So any analysis or prediction involving this would also likely be meaningless. For example, I can build a very accurate model that predicts the number of reviews a recipe will get by doing no calculations and just predicting 0 every time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Assessment of Missingness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how many missing data we have, as well as a breakdown of missing values in each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-31T23:36:28.662099Z",
     "start_time": "2019-10-31T23:36:28.660016Z"
    }
   },
   "outputs": [],
   "source": [
    "print('total missing values: ', recipes.isna().sum().sum())\n",
    "recipes.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('total missing values: ', interactions.isna().sum().sum())\n",
    "interactions.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll look at some of these. Firstly, let's look at the one missing name value in `recipes`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recipes[recipes['name'].isna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since it's only 1 missing value in this column out of hundreds of thousands of rows, doing a missingness analysis on this column would be pretty meaningless, and it would be negligible anyway."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another column in `recipes` with missing values is 'description'. We believe this is NMAR because if the user believes there is no need to describe the dish, then it will simply have no description and therefore be a missing value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we should consider the rating column. It has the most missing values out of all the columns. This makes sense because there are many people who write reviews or comments on the recipe without leaving a rating. Our guess is this is MCAR. We'll perform a permutation test to verify that. Our hypotheses are:\n",
    "- **Null Hypothesis**: The rating column is MCAR\n",
    "- **Alternative Hypothesis**: The rating column is not MCAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO : The thing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Hypothesis Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're interested in comparing American and Asian dishes. Specifically, we're concerned about health. Now, a healthy diet is usually a balanced diet, so we can't conclude one nutrient is objectively better to always have more of. But we can at the very least say saturated fat is objectively **bad** for you. Many national and international health organizations, such as [The American Heart Association](https://www.heart.org/en/healthy-living/healthy-eating/eat-smart/fats/saturated-fats) and [World Health Organization](https://www.who.int/news/item/17-07-2023-who-updates-guidelines-on-fats-and-carbohydrates) recommend either limiting or replacing saturated fat intake.<br><br>\n",
    "So to compare the healthiness of American and Asian dishes, we will be focusing on saturated fat content. We will do this comparison using a hypothesis test. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, some data wrangling. We need to extract the saturated fat from the nutrition column, which is currently a column of lists, with each list containing the values of various nutrients. We know from looking at the website that the saturated fat is the second last entry in each list, so we extract that and assign it to a new column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recipes['saturated_fat'] = recipes['nutrition'].apply(lambda x: float(x[-2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should keep in mind the saturated fat values are in percentages of daily value. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we assign labels to every row depending on if it's an American or Asian recipe. This information is stored in the tags, and all the tags are lowercase which makes our job easier. We assign a new column to see if the recipe is asian, american, or neither:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recipes['asian_or_american'] = recipes['tags'].apply(lambda x: 'asian' if 'asian' in x else 'american' if 'american' in x else 'neither')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we filter the dataset to only include Asian and American recipes. And we perform a permutation test on them. We name this dataframe `asia_america_recipes`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asia_america_recipes = recipes[recipes['asian_or_american']!='neither']\n",
    "asia_america_recipes.iloc[18:21]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a wrangled dataset, we can get to work constructing our hypothesis test. To decide our alternative hypothesis, we see which one currently has the higher mean saturated fat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_satfat_asia = asia_america_recipes[asia_america_recipes['asian_or_american']=='asian']['saturated_fat'].mean()\n",
    "mean_satfat_america = asia_america_recipes[asia_america_recipes['asian_or_american']=='american']['saturated_fat'].mean()\n",
    "print('Asian mean saturated fat: ', mean_satfat_asia, '\\nAmerican mean saturated fat: ', mean_satfat_america)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that American recipes have higher saturated fat on average. So that will be our alternative hypothesis. Our hypotheses are:\n",
    "- **Null Hypothesis**: American and Asian recipes on food.com have the same amount of saturated fat.\n",
    "- **Alternative Hypothesis**: American recipes have more saturated fat than Asian recipes.\n",
    "- Our test statistic will be `Mean saturated fat in American recipes` - `Mean saturated fat in Asian recipes`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-31T23:36:28.666489Z",
     "start_time": "2019-10-31T23:36:28.664381Z"
    }
   },
   "outputs": [],
   "source": [
    "observed_stat = mean_satfat_america - mean_satfat_asia\n",
    "\n",
    "num_simulations = 10000\n",
    "shuffled_df = asia_america_recipes.copy()\n",
    "simulated_stats = []\n",
    "\n",
    "for i in range(num_simulations):\n",
    "    shuffled_df['asian_or_american'] = np.random.permutation(shuffled_df['asian_or_american'])\n",
    "\n",
    "    shuffled_satfat_america = shuffled_df[shuffled_df['asian_or_american']=='american']['saturated_fat'].mean()\n",
    "    shuffled_satfat_asia = shuffled_df[shuffled_df['asian_or_american']=='asian']['saturated_fat'].mean()\n",
    "\n",
    "    one_sim_stat = shuffled_satfat_america-shuffled_satfat_asia\n",
    "    simulated_stats.append(one_sim_stat)\n",
    "\n",
    "simulated_stats = np.array(simulated_stats)\n",
    "p_value = np.count_nonzero(simulated_stats >= observed_stat)\n",
    "print('The p value is: ', p_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.histogram(simulated_stats)\n",
    "fig.add_vline(x=observed_stat, line_width=2,  line_color=\"red\")\n",
    "fig.add_annotation(\n",
    "    x=observed_stat,\n",
    "    y=1,\n",
    "    yref=\"paper\",\n",
    "    text=\"Observed statistic\",\n",
    "    showarrow=True,\n",
    "    arrowhead=1\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our p-value is 0. This means we can confidently reject the null hypothesis. We conclude that American recipes have more saturated fat than Asian recipes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For fun, we'll plot the distribution of the saturated fat in Asian recipes vs the distribution of saturated fat in American recipes to make sure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.histogram(asia_america_recipes[asia_america_recipes['asian_or_american']=='asian']['saturated_fat'])\n",
    "fig.data[0].name = 'Asia'\n",
    "fig.add_trace(\n",
    "    go.Histogram(\n",
    "        x=asia_america_recipes[asia_america_recipes['asian_or_american']=='american']['saturated_fat'],\n",
    "        opacity=0.7,\n",
    "        name='America'\n",
    "    )\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5.1: Framing a Prediction Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6.1: Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import make_scorer, mean_absolute_error, mean_squared_error, r2_score, f1_score, precision_score, recall_score, accuracy_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, FunctionTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "recipes['is_american'] = recipes['tags'].apply(lambda x: int('american' in x))\n",
    "\n",
    "X_train, X_test, y_train, y_test = (\n",
    "    train_test_split(recipes[[\"n_ingredients\", \"n_steps\", \"nutrition\"]]\n",
    "                     , recipes[\"is_american\"],\n",
    "                     random_state=12)\n",
    ")\n",
    "\n",
    "def extract_calories(nutrition_col):\n",
    "    return (nutrition_col.apply(lambda x: x[0]))\n",
    "\n",
    "#extracts calories from nutrition col\n",
    "nutrition_transformer = Pipeline([\n",
    "    (\"extract\", FunctionTransformer(lambda x: x.apply(extract_calories).values.reshape(-1, 1))),\n",
    "    (\"scale\", StandardScaler())  # Normalize extracted nutrition features\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    (\"num\", StandardScaler(), [\"n_ingredients\", \"n_steps\"]),\n",
    "    (\"nutrition\", nutrition_transformer, [\"nutrition\"])\n",
    "    ],\n",
    "remainder='passthrough')\n",
    "\n",
    "pl = Pipeline([\n",
    "    (\"preprocessor\", preprocessor),\n",
    "    (\"regressor\", DecisionTreeClassifier(max_depth=15,random_state=12))\n",
    "])\n",
    "\n",
    "model = pl.fit(X_train, y_train)\n",
    "print(\"Training Accuracy: \", model.score(X_train,y_train))\n",
    "print(\"Test Accuracy: \", model.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define K-Fold Cross-Validation\n",
    "kf = KFold(n_splits=5, shuffle=True)\n",
    "X = recipes[[\"n_ingredients\", \"n_steps\", \"nutrition\"]]\n",
    "y = recipes[\"is_american\"]\n",
    "\n",
    "# Define scoring metrics\n",
    "scoring = {\n",
    "    \"F1\": make_scorer(f1_score),\n",
    "    \"Precision\": make_scorer(precision_score),\n",
    "    \"Recall\": make_scorer(recall_score),\n",
    "    \"Accuracy\": make_scorer(accuracy_score)\n",
    "}\n",
    "\n",
    "# Perform cross-validation\n",
    "scores = {}\n",
    "for metric in scoring:\n",
    "    score = cross_val_score(pl, X, y, cv=kf, scoring=scoring[metric])\n",
    "    scores[metric] = score.mean()\n",
    "\n",
    "# Print results\n",
    "print(f\"Mean F1: {scores['F1']:.2f}\")\n",
    "print(f\"Mean Precision: {scores['Precision']:.2f}\")\n",
    "print(f\"Mean Recall: {scores['Recall']:.2f}\")\n",
    "print(f\"Mean Accuracy: {scores['Accuracy']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7.1: Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "X_train, X_test, y_train, y_test = (\n",
    "    train_test_split(recipes[[\"n_ingredients\", \"n_steps\", \"nutrition\", 'ingredients']], \n",
    "                     recipes[\"is_american\"],\n",
    "                    random_state=12)\n",
    ")\n",
    "\n",
    "target_ingredients = ['beef', 'pork', 'chicken', 'corn', 'potatoes', 'rice', 'bread', 'pasta',\n",
    "                      'milk', 'cheese', 'butter', 'sugar', 'flour', 'tomatoes', 'squash']\n",
    "\n",
    "def ingredient_onehot_encoder(X):\n",
    "    df_encoded = pd.DataFrame()\n",
    "    \n",
    "    for ingredient in target_ingredients:\n",
    "        df_encoded[ingredient] = X['ingredients'].apply(lambda x: int(any(ingredient in item for item in x)))\n",
    "\n",
    "    #df_encoded['is_american'] = X['tags'].apply(lambda x: int('american' in x))\n",
    "    return df_encoded\n",
    "\n",
    "def extract_calories(nutrition_col):\n",
    "    return (nutrition_col.apply(lambda x: x[0]))\n",
    "\n",
    "#extracts calories from nutrition col\n",
    "nutrition_transformer = Pipeline([\n",
    "    (\"extract\", FunctionTransformer(lambda x: x.apply(extract_calories).values.reshape(-1, 1))),\n",
    "    (\"scale\", StandardScaler())  # Normalize extracted nutrition features\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    (\"num\", StandardScaler(), [\"n_ingredients\", \"n_steps\"]),\n",
    "    (\"nutrition\", nutrition_transformer, [\"nutrition\"]),\n",
    "    ('onehot', FunctionTransformer(ingredient_onehot_encoder, validate=False), ['ingredients'])\n",
    "    ],\n",
    "remainder='passthrough')\n",
    "\n",
    "pl = Pipeline([\n",
    "    (\"preprocessor\", preprocessor),\n",
    "    (\"regressor\", RandomForestClassifier(max_depth=10,random_state=12))\n",
    "])\n",
    "\n",
    "model = pl.fit(X_train, y_train)\n",
    "print(\"Training Accuracy: \", model.score(X_train,y_train))\n",
    "print(\"Test Accuracy: \", model.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "hyperparameters = {\n",
    "    'regressor__n_estimators':  [10, 50, 100],\n",
    "    'regressor__max_depth': np.arange(2, 30, 10), \n",
    "    'regressor__criterion': ['gini', 'entropy']\n",
    "}\n",
    "grids = GridSearchCV(\n",
    "    pl,\n",
    "    n_jobs=-1, # Use multiple processors to parallelize\n",
    "    param_grid=hyperparameters,\n",
    "    return_train_score=True\n",
    ")\n",
    "grids.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grids.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grids.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = grids.predict(X_test)\n",
    "(y_pred==1).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_score(grids.predict(X_test),y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Framing a Prediction Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One challenge we face as college students is trying to manage time. So we decided to build a model that could predict the total cooking time of whatever one might want to cook.\n",
    "\n",
    "Our initial plan was to use a linear regression model, but the results weren't good as you will see later. Our final model will use a RandomForestRegressor. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Baseline Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our baseline model, our features will be number of ingredients, number of steps, and calories per serving. First, we import the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-31T23:36:28.662099Z",
     "start_time": "2019-10-31T23:36:28.660016Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import make_scorer, mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, FunctionTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The amount of calories is still stored in the `nutrition` column, so we extract that information and assign it to a new column `calories`. We will also remove some outliers. We choose to remove recipes that take more than 3 hours to make and remove recipes that have calories equal to or over 2000, since that's the recommended daily calorie intake of an adult male."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recipes['calories'] = recipes['nutrition'].apply(lambda x: float(x[0]))\n",
    "recipes_no_outliers = recipes[(recipes['minutes'] < 180) & (recipes['calories']<2000)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's do some scatterplots to get an idea of the fit of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.scatter(recipes_no_outliers, x='n_steps', y='minutes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.scatter(recipes_no_outliers, x='n_ingredients', y='minutes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.scatter(recipes_no_outliers, x='calories', y='minutes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It turns out the data has no clear pattern, so a linear regression probably won't do well. We'll try it out anyway."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = (\n",
    "    train_test_split(recipes_no_outliers[[\"n_ingredients\", \"n_steps\", \"nutrition\"]], recipes_no_outliers[\"minutes\"], random_state=1)\n",
    ")\n",
    "\n",
    "\n",
    "def extract_calories(nutrition_col):\n",
    "    return (nutrition_col.apply(lambda x: x[0]))\n",
    "\n",
    "#extracts calories from nutrition col\n",
    "nutrition_transformer = Pipeline([\n",
    "    (\"extract\", FunctionTransformer(lambda x: x.apply(extract_calories).values.reshape(-1, 1))),\n",
    "    (\"scale\", StandardScaler())  # Normalize extracted nutrition features\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    (\"num\", StandardScaler(), [\"n_ingredients\", \"n_steps\"]),\n",
    "    (\"nutrition\", nutrition_transformer, [\"nutrition\"])\n",
    "    ],\n",
    "remainder='passthrough')\n",
    "\n",
    "pl = Pipeline([\n",
    "    (\"preprocessor\", preprocessor),\n",
    "    (\"regressor\", LinearRegression())\n",
    "])\n",
    "\n",
    "model = pl.fit(X_train, y_train)\n",
    "print(\"Training R^2: \", model.score(X_train,y_train))\n",
    "print(\"Test R^2: \", model.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see it performs pretty badly. Not surprising considering how the scatterplots looked. So we choose to use a decision tree instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "X_train, X_test, y_train, y_test = (\n",
    "    train_test_split(recipes_no_outliers[[\"n_ingredients\", \"n_steps\", \"nutrition\"]], recipes_no_outliers[\"minutes\"], random_state=1)\n",
    ")\n",
    "\n",
    "\n",
    "def extract_calories(nutrition_col):\n",
    "    return (nutrition_col.apply(lambda x: x[0]))\n",
    "\n",
    "#extracts calories from nutrition col\n",
    "nutrition_transformer = Pipeline([\n",
    "    (\"extract\", FunctionTransformer(lambda x: x.apply(extract_calories).values.reshape(-1, 1))),\n",
    "    (\"scale\", StandardScaler())  # Normalize extracted nutrition features\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    (\"num\", StandardScaler(), [\"n_ingredients\", \"n_steps\"]),\n",
    "    (\"nutrition\", nutrition_transformer, [\"nutrition\"])\n",
    "    ],\n",
    "remainder='passthrough')\n",
    "\n",
    "pl = Pipeline([\n",
    "    (\"preprocessor\", preprocessor),\n",
    "    (\"regressor\", DecisionTreeRegressor(max_depth=15,random_state=42))\n",
    "])\n",
    "\n",
    "model = pl.fit(X_train, y_train)\n",
    "print(\"Training R^2: \", model.score(X_train,y_train))\n",
    "print(\"Test R^2: \", model.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's evaluate our model using K-Fold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define K-Fold Cross-Validation\n",
    "kf = KFold(n_splits=5, shuffle=True)\n",
    "X = recipes_no_outliers[[\"n_ingredients\", \"n_steps\", \"nutrition\"]]\n",
    "y = recipes_no_outliers[\"minutes\"]\n",
    "\n",
    "# Define scoring metrics\n",
    "scoring = {\n",
    "    \"MAE\": make_scorer(mean_absolute_error),\n",
    "    \"MSE\": make_scorer(mean_squared_error),\n",
    "    \"R2\": make_scorer(r2_score)\n",
    "}\n",
    "\n",
    "# Perform cross-validation\n",
    "scores = {}\n",
    "for metric in scoring:\n",
    "    score = cross_val_score(pl, X, y, cv=kf, scoring=scoring[metric])\n",
    "    scores[metric] = score.mean()\n",
    "\n",
    "# Compute RMSE separately since it's the square root of MSE\n",
    "rmse_scores = np.sqrt(-cross_val_score(pl, X, y, cv=kf, scoring=\"neg_mean_squared_error\"))\n",
    "\n",
    "# Print results\n",
    "print(f\"Mean MAE: {scores['MAE']:.2f}\")\n",
    "print(f\"Mean MSE: {scores['MSE']:.2f}\")\n",
    "print(f\"Mean RMSE: {rmse_scores.mean():.2f}\")\n",
    "print(f\"Mean RÂ² Score: {scores['R2']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Still not very good. To improve our final mode we'll use a random forest to avoid overfitting and also GridSearchCV to tune our hyperparameters. We'll also include more features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Final Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll do a random forest and use GridsearchCV to fine tune our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recipes['asian_or_american'] = recipes['tags'].apply(lambda x: 'american' if 'american' in x )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-31T23:36:28.662099Z",
     "start_time": "2019-10-31T23:36:28.660016Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "X_train, X_test, y_train, y_test = (\n",
    "    train_test_split(recipes_no_outliers[[\"n_ingredients\", \"n_steps\", \"nutrition\", 'ingredients']], recipes_no_outliers[\"minutes\"], random_state=1)\n",
    ")\n",
    "\n",
    "target_ingredients = ['beef', 'pork', 'chicken', 'corn', 'potatoes', 'rice', 'bread', 'pasta',\n",
    "                      'milk', 'cheese', 'butter', 'sugar', 'flour', 'tomatoes', 'squash']\n",
    "\n",
    "def ingredient_onehot_encoder(X):\n",
    "    df_encoded = pd.DataFrame()\n",
    "    \n",
    "    for ingredient in target_ingredients:\n",
    "        df_encoded[ingredient] = X['ingredients'].apply(lambda x: int(any(ingredient in item for item in x)))\n",
    "\n",
    "    #df_encoded['is_american'] = X['tags'].apply(lambda x: int('american' in x))\n",
    "    return df_encoded\n",
    "\n",
    "def extract_calories(nutrition_col):\n",
    "    return (nutrition_col.apply(lambda x: x[0]))\n",
    "\n",
    "#extracts calories from nutrition col\n",
    "nutrition_transformer = Pipeline([\n",
    "    (\"extract\", FunctionTransformer(lambda x: x.apply(extract_calories).values.reshape(-1, 1))),\n",
    "    (\"scale\", StandardScaler())  # Normalize extracted nutrition features\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    (\"num\", StandardScaler(), [\"n_ingredients\", \"n_steps\"]),\n",
    "    (\"nutrition\", nutrition_transformer, [\"nutrition\"]),\n",
    "    ('onehot', FunctionTransformer(ingredient_onehot_encoder, validate=False), ['ingredients'])\n",
    "    ],\n",
    "remainder='passthrough')\n",
    "\n",
    "pl = Pipeline([\n",
    "    (\"preprocessor\", preprocessor),\n",
    "    (\"regressor\", RandomForestRegressor(max_depth=10,random_state=42))\n",
    "])\n",
    "\n",
    "model = pl.fit(X_train, y_train)\n",
    "print(\"Training R^2: \", model.score(X_train,y_train))\n",
    "print(\"Test R^2: \", model.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "hyperparameters = {\n",
    "    'regressor__n_estimators':  [10, 50, 100],\n",
    "    'regressor__max_depth': np.arange(2, 30, 10), \n",
    "    'regressor__criterion': ['squared_error', 'friedman_mse', 'poisson']\n",
    "}\n",
    "grids = GridSearchCV(\n",
    "    pl,\n",
    "    n_jobs=-1, # Use multiple processors to parallelize\n",
    "    param_grid=hyperparameters,\n",
    "    return_train_score=True\n",
    ")\n",
    "grids.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grids.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grids.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Fairness Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-31T23:36:28.666489Z",
     "start_time": "2019-10-31T23:36:28.664381Z"
    }
   },
   "outputs": [],
   "source": [
    "# TODO"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsc80",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
