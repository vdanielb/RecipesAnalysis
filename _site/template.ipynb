{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recipes Analysis\n",
    "\n",
    "**Name(s)**: Daniel Budidharma, Tristan Leo\n",
    "\n",
    "**Website Link**: https://vdanielb.github.io/RecipesAnalysis/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-31T23:36:28.652554Z",
     "start_time": "2019-10-31T23:36:27.180520Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "pd.options.plotting.backend = 'plotly'\n",
    "\n",
    "# from dsc80_utils import * # Feel free to uncomment and use this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's load in the dataset and take a look at it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recipes = pd.read_csv('data/RAW_recipes.csv')\n",
    "interactions = pd.read_csv('data/RAW_interactions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(recipes.head())\n",
    "display(interactions.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Data Cleaning and Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first remove unnamed: 0 from our `recipes` dataframe. That is just the index number on the original dataset before we took a subset of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recipes = recipes.drop(columns=[\"Unnamed: 0\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at a particular row in `interactions`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(interactions.iloc[3:4])\n",
    "print(interactions['review'].iloc[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the lowest possible rating a user could give is 1 star. So how does this recipe have a rating of 0? It turns out that that means the reviewer just didn't leave a rating. Like the review in this particular row says, \"...so I will not rate\". It makes sense then to replace these values with NaN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactions['rating'] = interactions['rating'].replace(0, np.nan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another thing we should notice is that the values in the tags column in `recipes` isn't actually a list. This is also true for other columns with values that look like lists. They're actually strings! To convert them into a list, we define a function and apply it to all those columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_col_string_to_list(df, col):\n",
    "    translation_table = str.maketrans({\"[\": \"\", \n",
    "                                   \"]\": \"\",\n",
    "                                    \"\\'\":\"\"})\n",
    "    df[col] = df[col].str.translate(translation_table).str.split(', ')\n",
    "\n",
    "for col in ['tags','nutrition', 'steps', 'ingredients']:\n",
    "    convert_col_string_to_list(recipes, col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's verify they're lists now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The type of the value is: \",type(recipes['tags'].iloc[4268]))\n",
    "(recipes['tags'].iloc[4268])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can actually perform list operations on those columns. Next, we're interested in finding the average rating per recipe. To do that we'll first have to merge the recipes and ratings dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recipes_with_ratings = recipes.merge(interactions, left_on='id', right_on='recipe_id',how='left')\n",
    "recipes_with_ratings.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`recipes_with_ratings` is now a dataframe with multiple rows for a single recipe, each row corresponding to a review for that recipe. If it has no reviews, then the columns associated with a review should be NaN. Now let's compute the average rating per recipe and include that in our original `recipes` dataframe, no duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recipes_with_ratings['average_rating'] = recipes_with_ratings.groupby('id')['rating'].transform(lambda x: x.mean())\n",
    "recipes = recipes_with_ratings.drop_duplicates(subset='id')\n",
    "recipes = recipes.drop(columns=['user_id', 'date', 'recipe_id','rating','review'])\n",
    "print(recipes.shape)\n",
    "recipes.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can start on some EDA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distribution of ratings should theoretically look something like a normal distribution, with most people rating 3 stars for average satisfaction, while few people would have extreme experiences that would warrant a 5 star or 1 star. Does our ratings column look like a normal distribution? Let's check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.histogram(recipes, x=\"average_rating\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Surprisingly a lot of 5s. Does this mean every recipe on food.com is a masterpiece? Probably not. It just means people are generous with ratings. It also might mean recipes that would've been rated low just don't get reviewed as much as recipes that are rated high. This makes sense, higher reviews lead to more views which lead to even more reviews.  \n",
    "<br> Still, this isn't good because it means the average rating doesn't tell us much about the actual quality of the recipe compared to other recipes. If everything is 5 stars, how do I know which recipe is better than the other? It is for this reason that we think any analysis involving the average rating probably won't be very useful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do something similar with number of reviews of each recipe. We define a function to get the number of reviews of each recipe id. And then we plot a histogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_reviews(id):\n",
    "    return interactions[interactions['user_id'] == id].shape[0]\n",
    "recipes['num_reviews'] = recipes['id'].apply(get_num_reviews)\n",
    "px.histogram(recipes['num_reviews'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, an overwhelming majority of recipes have 0 reviews. So any analysis or prediction involving this would also likely be meaningless. For example, I can build a very accurate model that predicts the number of reviews a recipe will get by doing no calculations and just predicting 0 every time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Assessment of Missingness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how many missing data we have, as well as a breakdown of missing values in each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-31T23:36:28.662099Z",
     "start_time": "2019-10-31T23:36:28.660016Z"
    }
   },
   "outputs": [],
   "source": [
    "print('total missing values: ', recipes.isna().sum().sum())\n",
    "recipes.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('total missing values: ', interactions.isna().sum().sum())\n",
    "interactions.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll look at some of these. Firstly, let's look at the one missing name value in `recipes`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recipes[recipes['name'].isna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since it's only 1 missing value in this column out of hundreds of thousands of rows, doing a missingness analysis on this column would be pretty meaningless, and it would be negligible anyway."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another column in `recipes` with missing values is 'description'. We believe this is NMAR because if the user believes there is no need to describe the dish, then it will simply have no description and therefore be a missing value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we should consider the rating column. It has the most missing values out of all the columns. This makes sense because there are many people who write reviews or comments on the recipe without leaving a rating. Our guess is this is MCAR. We'll perform a permutation test to verify that. Our hypotheses are:\n",
    "- **Null Hypothesis**: The rating column is MCAR\n",
    "- **Alternative Hypothesis**: The rating column is not MCAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO : The thing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Hypothesis Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're interested in comparing American and Asian dishes. Specifically, we're concerned about health. Now, a healthy diet is usually a balanced diet, so we can't conclude one nutrient is objectively better to always have more of. But we can at the very least say saturated fat is objectively **bad** for you. Many national and international health organizations, such as [The American Heart Association](https://www.heart.org/en/healthy-living/healthy-eating/eat-smart/fats/saturated-fats) and [World Health Organization](https://www.who.int/news/item/17-07-2023-who-updates-guidelines-on-fats-and-carbohydrates) recommend either limiting or replacing saturated fat intake.<br><br>\n",
    "So to compare the healthiness of American and Asian dishes, we will be focusing on saturated fat content. We will do this comparison using a hypothesis test. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, some data wrangling. We need to extract the saturated fat from the nutrition column, which is currently a column of lists, with each list containing the values of various nutrients. We know from looking at the website that the saturated fat is the second last entry in each list, so we extract that and assign it to a new column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recipes['saturated_fat'] = recipes['nutrition'].apply(lambda x: float(x[-2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should keep in mind the saturated fat values are in percentages of daily value. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we assign labels to every row depending on if it's an American or Asian recipe. This information is stored in the tags, and all the tags are lowercase which makes our job easier. We assign a new column to see if the recipe is asian, american, or neither:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recipes['asian_or_american'] = recipes['tags'].apply(lambda x: 'asian' if 'asian' in x else 'american' if 'american' in x else 'neither')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we filter the dataset to only include Asian and American recipes. And we perform a permutation test on them. We name this dataframe `asia_america_recipes`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asia_america_recipes = recipes[recipes['asian_or_american']!='neither']\n",
    "asia_america_recipes.iloc[18:21]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a wrangled dataset, we can get to work constructing our hypothesis test. To decide our alternative hypothesis, we see which one currently has the higher mean saturated fat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_satfat_asia = asia_america_recipes[asia_america_recipes['asian_or_american']=='asian']['saturated_fat'].mean()\n",
    "mean_satfat_america = asia_america_recipes[asia_america_recipes['asian_or_american']=='american']['saturated_fat'].mean()\n",
    "print('Asian mean saturated fat: ', mean_satfat_asia, '\\nAmerican mean saturated fat: ', mean_satfat_america)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that American recipes have higher saturated fat on average. So that will be our alternative hypothesis. Our hypotheses are:\n",
    "- **Null Hypothesis**: American and Asian recipes on food.com have the same amount of saturated fat.\n",
    "- **Alternative Hypothesis**: American recipes have more saturated fat than Asian recipes.\n",
    "- Our test statistic will be `Mean saturated fat in American recipes` - `Mean saturated fat in Asian recipes`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-31T23:36:28.666489Z",
     "start_time": "2019-10-31T23:36:28.664381Z"
    }
   },
   "outputs": [],
   "source": [
    "observed_stat = mean_satfat_america - mean_satfat_asia\n",
    "\n",
    "num_simulations = 10000\n",
    "shuffled_df = asia_america_recipes.copy()\n",
    "simulated_stats = []\n",
    "\n",
    "for i in range(num_simulations):\n",
    "    shuffled_df['asian_or_american'] = np.random.permutation(shuffled_df['asian_or_american'])\n",
    "\n",
    "    shuffled_satfat_america = shuffled_df[shuffled_df['asian_or_american']=='american']['saturated_fat'].mean()\n",
    "    shuffled_satfat_asia = shuffled_df[shuffled_df['asian_or_american']=='asian']['saturated_fat'].mean()\n",
    "\n",
    "    one_sim_stat = shuffled_satfat_america-shuffled_satfat_asia\n",
    "    simulated_stats.append(one_sim_stat)\n",
    "\n",
    "simulated_stats = np.array(simulated_stats)\n",
    "p_value = np.count_nonzero(simulated_stats >= observed_stat)\n",
    "print('The p value is: ', p_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.histogram(simulated_stats)\n",
    "fig.add_vline(x=observed_stat, line_width=2,  line_color=\"red\")\n",
    "fig.add_annotation(\n",
    "    x=observed_stat,\n",
    "    y=1,\n",
    "    yref=\"paper\",\n",
    "    text=\"Observed statistic\",\n",
    "    showarrow=True,\n",
    "    arrowhead=1\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our p-value is 0. This means we can confidently reject the null hypothesis. We conclude that American recipes have more saturated fat than Asian recipes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For fun, we'll plot the distribution of the saturated fat in Asian recipes vs the distribution of saturated fat in American recipes to make sure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.histogram(asia_america_recipes[asia_america_recipes['asian_or_american']=='asian']['saturated_fat'])\n",
    "fig.data[0].name = 'Asia'\n",
    "fig.add_trace(\n",
    "    go.Histogram(\n",
    "        x=asia_america_recipes[asia_america_recipes['asian_or_american']=='american']['saturated_fat'],\n",
    "        opacity=0.7,\n",
    "        name='America'\n",
    "    )\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5.1: Framing a Prediction Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our original plan was to predict if a recipe was American or not American based on nutrition, n_ingredients, n_steps. However, this proved to be uninteresting. While our model did reach an accuracy of 88.9%, our recall, precision, and F1 score were 0. After further investigation, it seems it's because our model guessed 0 (not American) every time. This is due to how most of recipes are not American, so it makes sense that our model would want to predict not American every time to maximize accuracy. While this did make the model more accurate, it made the F1 score very low. We ultimately decided not to continue with this prediction problem because making a model that only predicted one thing every time isn't interesting at all, even if it is highly accurate. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6.1: Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import make_scorer, mean_absolute_error, mean_squared_error, r2_score, f1_score, precision_score, recall_score, accuracy_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, FunctionTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "recipes['is_american'] = recipes['tags'].apply(lambda x: 1 if 'american' in x else 0)\n",
    "X_train, X_test, y_train, y_test = (\n",
    "    train_test_split(recipes[[\"n_ingredients\", \"n_steps\", \"nutrition\"]]\n",
    "                     , recipes[\"is_american\"],\n",
    "                     random_state=12)\n",
    ")\n",
    "\n",
    "def extract_calories(nutrition_col):\n",
    "    return (nutrition_col.apply(lambda x: x[0]))\n",
    "\n",
    "#extracts calories from nutrition col\n",
    "nutrition_transformer = Pipeline([\n",
    "    (\"extract\", FunctionTransformer(lambda x: x.apply(extract_calories).values.reshape(-1, 1))),\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    (\"nutrition\", nutrition_transformer, [\"nutrition\"])\n",
    "    ],\n",
    "remainder='passthrough')\n",
    "\n",
    "pl = Pipeline([\n",
    "    (\"preprocessor\", preprocessor),\n",
    "    (\"regressor\", DecisionTreeClassifier(max_depth=15,random_state=12))\n",
    "])\n",
    "\n",
    "model = pl.fit(X_train, y_train)\n",
    "print(\"Training Accuracy: \", model.score(X_train,y_train))\n",
    "print(\"Test Accuracy: \", model.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define K-Fold Cross-Validation\n",
    "kf = KFold(n_splits=5, shuffle=True)\n",
    "X = recipes[[\"n_ingredients\", \"n_steps\", \"nutrition\"]]\n",
    "y = recipes[\"is_american\"]\n",
    "\n",
    "# Define scoring metrics\n",
    "scoring = {\n",
    "    \"F1\": make_scorer(f1_score),\n",
    "    \"Precision\": make_scorer(precision_score),\n",
    "    \"Recall\": make_scorer(recall_score),\n",
    "    \"Accuracy\": make_scorer(accuracy_score)\n",
    "}\n",
    "\n",
    "# Perform cross-validation\n",
    "scores = {}\n",
    "for metric in scoring:\n",
    "    score = cross_val_score(pl, X, y, cv=kf, scoring=scoring[metric])\n",
    "    scores[metric] = score.mean()\n",
    "\n",
    "# Print results\n",
    "print(f\"Mean F1: {scores['F1']:.2f}\")\n",
    "print(f\"Mean Precision: {scores['Precision']:.2f}\")\n",
    "print(f\"Mean Recall: {scores['Recall']:.2f}\")\n",
    "print(f\"Mean Accuracy: {scores['Accuracy']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7.1: Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "X_train, X_test, y_train, y_test = (\n",
    "    train_test_split(recipes[[\"n_ingredients\", \"n_steps\", \"nutrition\", 'ingredients']], \n",
    "                     recipes[\"is_american\"],\n",
    "                    random_state=12)\n",
    ")\n",
    "\n",
    "\n",
    "def ingredient_onehot_encoder(X):\n",
    "    target_ingredients = ['beef', 'pork', 'chicken', 'corn', 'potatoes', 'rice', 'bread', 'pasta',\n",
    "                      'milk', 'cheese', 'butter', 'sugar', 'flour', 'tomatoes', 'squash']\n",
    "    df_encoded = pd.DataFrame()\n",
    "    \n",
    "    for ingredient in target_ingredients:\n",
    "        df_encoded[ingredient] = X['ingredients'].apply(lambda x: int(any(ingredient in item for item in x)))\n",
    "\n",
    "    return df_encoded\n",
    "\n",
    "def extract_nutrients(X):\n",
    "    nutrition_features = ['calories', 'total_fat', 'sugar', 'sodium', \n",
    "                          'protein', 'saturated_fat', 'carbohydrates']\n",
    "    df = pd.DataFrame()\n",
    "    for i,nutrition in enumerate(nutrition_features):\n",
    "        df[nutrition] = X['nutrition'].apply(lambda x: x[i])\n",
    "\n",
    "    return df\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('nutrition', FunctionTransformer(extract_nutrients, validate=False), ['nutrition']),\n",
    "    ('onehot', FunctionTransformer(ingredient_onehot_encoder, validate=False), ['ingredients'])\n",
    "    ],\n",
    "remainder='passthrough')\n",
    "\n",
    "pl = Pipeline([\n",
    "    (\"preprocessor\", preprocessor),\n",
    "    (\"regressor\", RandomForestClassifier(max_depth=10,random_state=12))\n",
    "])\n",
    "\n",
    "model = pl.fit(X_train, y_train)\n",
    "print(\"Training Accuracy: \", model.score(X_train,y_train))\n",
    "print(\"Test Accuracy: \", model.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.get_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "hyperparameters = {\n",
    "    'regressor__n_estimators':  [10, 50, 100],\n",
    "    'regressor__max_depth': np.arange(2, 30, 10), \n",
    "    'regressor__criterion': ['gini', 'entropy']\n",
    "}\n",
    "grids = GridSearchCV(\n",
    "    pl,\n",
    "    n_jobs=-1, # Use multiple processors to parallelize\n",
    "    param_grid=hyperparameters,\n",
    "    return_train_score=True\n",
    ")\n",
    "grids.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = grids.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"accuracy: \", grids.score(X_test, y_test))\n",
    "print(\"precision: \", precision_score(y_pred,y_test))\n",
    "print(\"recall: \", recall_score(y_pred,y_test))\n",
    "print(\"f1: \", f1_score(y_pred,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grids.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While accuracy did go up by 0.9%, our precision, recall, and F1 score became 0. After further investigation, it seems it's because our model guesses 0 (not American) every time, shown by how the sum of y_pred is 0. This is due to how most of the data is not American, so it makes sense that our model would want to predict not American every time to maximize accuracy. While this does make the model more accurate, it makes our F1 score very low. We ultimately decided not to continue with this prediction problem because making a model that only predicts one thing every time isn't interesting at all, even if it is highly accurate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Framing a Prediction Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One challenge we face as college students is trying to manage time. So we decided to build a model that could predict the total cooking time of whatever one might want to cook.\n",
    "\n",
    "Our initial plan was to use a linear regression model, but the results weren't good as you will see later. Our final model will use a RandomForestRegressor. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Baseline Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our baseline model, our features will be number of ingredients, number of steps, and calories per serving. First, we import the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-31T23:36:28.662099Z",
     "start_time": "2019-10-31T23:36:28.660016Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import make_scorer, mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, FunctionTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The amount of calories is still stored in the `nutrition` column, so we extract that information and assign it to a new column `calories`. We will also remove some outliers. We choose to remove recipes that take more than 3 hours to make and remove recipes that have calories equal to or over 2000, since that's the recommended daily calorie intake of an adult male."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recipes['calories'] = recipes['nutrition'].apply(lambda x: float(x[0]))\n",
    "recipes_no_outliers = recipes[(recipes['minutes'] < 180) & (recipes['calories']<2000)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's do some scatterplots to get an idea of the fit of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.scatter(recipes_no_outliers, x='n_steps', y='minutes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.scatter(recipes_no_outliers, x='n_ingredients', y='minutes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.scatter(recipes_no_outliers, x='calories', y='minutes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It turns out the data has no clear pattern, so a linear regression probably won't do well. We'll try it out anyway."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = (\n",
    "    train_test_split(recipes_no_outliers[[\"n_ingredients\", \"n_steps\", \"nutrition\"]], recipes_no_outliers[\"minutes\"], random_state=1)\n",
    ")\n",
    "\n",
    "\n",
    "def extract_calories(nutrition_col):\n",
    "    return (nutrition_col.apply(lambda x: x[0]))\n",
    "\n",
    "#extracts calories from nutrition col\n",
    "nutrition_transformer = Pipeline([\n",
    "    (\"extract\", FunctionTransformer(lambda x: x.apply(extract_calories).values.reshape(-1, 1))),\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    (\"nutrition\", nutrition_transformer, [\"nutrition\"])\n",
    "    ],\n",
    "remainder='passthrough')\n",
    "\n",
    "pl = Pipeline([\n",
    "    (\"preprocessor\", preprocessor),\n",
    "    (\"regressor\", LinearRegression())\n",
    "])\n",
    "\n",
    "model = pl.fit(X_train, y_train)\n",
    "print(\"Training R^2: \", model.score(X_train,y_train))\n",
    "print(\"Test R^2: \", model.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see it performs pretty badly. Not surprising considering how the scatterplots looked. So we choose to use a decision tree instead. We will set max_depth = 10 to avoid overfitting and set random_state=12 for reproducability. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "X_train, X_test, y_train, y_test = (\n",
    "    train_test_split(recipes_no_outliers[[\"n_ingredients\", \"n_steps\", \"nutrition\"]], \n",
    "                     recipes_no_outliers[\"minutes\"], random_state=12)\n",
    ")\n",
    "\n",
    "\n",
    "def extract_calories(nutrition_col):\n",
    "    return (nutrition_col.apply(lambda x: x[0]))\n",
    "\n",
    "#extracts calories from nutrition col\n",
    "nutrition_transformer = Pipeline([\n",
    "    (\"extract\", FunctionTransformer(lambda x: x.apply(extract_calories).values.reshape(-1, 1))),\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    (\"nutrition\", nutrition_transformer, [\"nutrition\"])\n",
    "    ],\n",
    "remainder='passthrough')\n",
    "\n",
    "pl = Pipeline([\n",
    "    (\"preprocessor\", preprocessor),\n",
    "    (\"regressor\", DecisionTreeRegressor(max_depth=10,random_state=12))\n",
    "])\n",
    "\n",
    "model = pl.fit(X_train, y_train)\n",
    "print(\"Training R^2: \", model.score(X_train,y_train))\n",
    "print(\"Test R^2: \", model.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's evaluate our model using K-Fold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define K-Fold Cross-Validation\n",
    "kf = KFold(n_splits=5, shuffle=True)\n",
    "X = recipes_no_outliers[[\"n_ingredients\", \"n_steps\", \"nutrition\"]]\n",
    "y = recipes_no_outliers[\"minutes\"]\n",
    "\n",
    "# Define scoring metrics\n",
    "scoring = {\n",
    "    \"MAE\": make_scorer(mean_absolute_error),\n",
    "    \"MSE\": make_scorer(mean_squared_error),\n",
    "    \"R2\": make_scorer(r2_score)\n",
    "}\n",
    "\n",
    "# Perform cross-validation\n",
    "scores = {}\n",
    "for metric in scoring:\n",
    "    score = cross_val_score(pl, X, y, cv=kf, scoring=scoring[metric])\n",
    "    scores[metric] = score.mean()\n",
    "\n",
    "# Compute RMSE separately since it's the square root of MSE\n",
    "rmse_scores = np.sqrt(-cross_val_score(pl, X, y, cv=kf, scoring=\"neg_mean_squared_error\"))\n",
    "\n",
    "# Print results\n",
    "print(f\"Mean MAE: {scores['MAE']:.2f}\")\n",
    "print(f\"Mean MSE: {scores['MSE']:.2f}\")\n",
    "print(f\"Mean RMSE: {rmse_scores.mean():.2f}\")\n",
    "print(f\"Mean R² Score: {scores['R2']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Still not very good. To improve our final mode we'll use a random forest to avoid overfitting and also GridSearchCV to tune our hyperparameters. We'll also include more features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Final Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll do a random forest and use GridsearchCV to fine tune our model.  \n",
    "  \n",
    "Furthermore, we're going to one hot encode a list ingredients. There are too many unique ingredients in this whole dataset to feasibly one hot encode, so we'll focus on a few common ingredients.  That is, we're gonna feature engineer if a recipe contains these ingredients: ['beef', 'pork', 'chicken', 'corn', 'potatoes', 'rice', 'bread', 'pasta', 'milk', 'cheese', 'butter', 'sugar', 'flour', 'tomatoes', 'squash']. Since there could be many types of the same ingredient (e.g. sweet corn vs normal corn, unsalted butter vs salted butter), we will make it so that any instance of that word appearing in the ingredients column means the ingredient is present. For example, if a recipe has 'sweet corn' as an ingredient, we consider that as containing corn\n",
    "  \n",
    "And also, we're going to feature engineer more nutrition columns. That is, instead of just using calories, we'll also use 'total_fat', 'sugar', 'sodium', 'protein', 'saturated_fat', and 'carbohydrates' in our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-31T23:36:28.662099Z",
     "start_time": "2019-10-31T23:36:28.660016Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "X_train, X_test, y_train, y_test = (\n",
    "    train_test_split(recipes_no_outliers[[\"n_ingredients\", \"n_steps\", \"nutrition\", 'ingredients']], \n",
    "                     recipes_no_outliers[\"minutes\"], \n",
    "                     random_state=12)\n",
    ")\n",
    "\n",
    "target_ingredients = ['beef', 'pork', 'chicken', 'corn', 'potatoes', 'rice', 'bread', 'pasta',\n",
    "                      'milk', 'cheese', 'butter', 'sugar', 'flour', 'tomatoes', 'squash']\n",
    "\n",
    "def ingredient_onehot_encoder(X):\n",
    "    df_encoded = pd.DataFrame()\n",
    "    \n",
    "    for ingredient in target_ingredients:\n",
    "        df_encoded[ingredient] = X['ingredients'].apply(lambda x: int(any(ingredient in item for item in x)))\n",
    "\n",
    "    return df_encoded\n",
    "\n",
    "def extract_nutrients(X):\n",
    "    nutrition_features = ['calories', 'total_fat', 'sugar', 'sodium', \n",
    "                          'protein', 'saturated_fat', 'carbohydrates']\n",
    "    df = pd.DataFrame()\n",
    "    for i,nutrition in enumerate(nutrition_features):\n",
    "        df[nutrition] = X['nutrition'].apply(lambda x: x[i])\n",
    "\n",
    "    return df\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('nutrition', FunctionTransformer(extract_nutrients, validate=False), ['nutrition']),\n",
    "    ('onehot', FunctionTransformer(ingredient_onehot_encoder, validate=False), ['ingredients'])\n",
    "    ],\n",
    "remainder='passthrough')\n",
    "\n",
    "pl = Pipeline([\n",
    "    (\"preprocessor\", preprocessor),\n",
    "    (\"regressor\", RandomForestRegressor(max_depth=10,random_state=12))\n",
    "])\n",
    "\n",
    "model = pl.fit(X_train, y_train)\n",
    "print(\"Training R^2: \", model.score(X_train,y_train))\n",
    "print(\"Test R^2: \", model.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "hyperparameters = {\n",
    "    'regressor__n_estimators':  [10, 50, 100],\n",
    "    'regressor__max_depth': np.arange(2, 30, 10), \n",
    "    'regressor__criterion': ['squared_error', 'friedman_mse', 'poisson']\n",
    "}\n",
    "grids = GridSearchCV(\n",
    "    pl,\n",
    "    n_jobs=-1, # Use multiple processors to parallelize\n",
    "    param_grid=hyperparameters,\n",
    "    return_train_score=True\n",
    ")\n",
    "grids.fit(X_train, y_train)\n",
    "print(\"R^2: \", grids.score(X_test, y_test))\n",
    "print(\"RMSE: \", np.sqrt(mean_squared_error(y_pred=grids.predict(X_test), y_true=y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grids.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad. It is at this point we suspect that our fit is really bad because of bad data quality. Some people on food.com could just upload random recipes, with random n_steps, random minutes, random ingredients, etc. and there's no good quality check. So we'll try to filter recipes so it will only have \"good quality\" data points. We decide a data point is of \"good quality\" if it has an average rating >=4. From the EDA, we saw there are a lot of recipes with an average rating above 4, so this shouldn't hurt our sample size too much. The pipeline still still be the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_recipes = recipes_no_outliers[recipes_no_outliers[\"average_rating\"] >= 4.5]\n",
    "good_recipes.shape #still 55818 rows!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_good, X_test_good, y_train_good, y_test_good = (\n",
    "    train_test_split(good_recipes[[\"n_ingredients\", \"n_steps\", \"nutrition\", 'ingredients']], \n",
    "                     good_recipes[\"minutes\"], \n",
    "                     random_state=12)\n",
    ")\n",
    "\n",
    "model_good = pl.fit(X_train_good, y_train_good)\n",
    "print(\"Training R^2: \", model_good.score(X_train_good,y_train_good))\n",
    "print(\"Test R^2: \", model_good.score(X_test_good,y_test_good))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grids_good = GridSearchCV(\n",
    "    pl,\n",
    "    n_jobs=-1, # Use multiple processors to parallelize\n",
    "    param_grid=hyperparameters,\n",
    "    return_train_score=True\n",
    ")\n",
    "grids_good.fit(X_train_good, y_train_good)\n",
    "print(\"r2: \", grids.score(X_test_good, y_test_good))\n",
    "print(\"RMSE: \", np.sqrt(mean_squared_error(y_pred=grids.predict(X_test_good), y_true=y_test_good)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grids_good.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Fairness Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-31T23:36:28.666489Z",
     "start_time": "2019-10-31T23:36:28.664381Z"
    }
   },
   "outputs": [],
   "source": [
    "# TODO"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsc80",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
